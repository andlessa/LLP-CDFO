#!/bin/python3

from pathlib import Path
import pandas as pd
import numpy as np
import glob
import re
import SAFreader as sfr
import pickle

if __name__ == '__main__':
	import argparse
	parser = argparse.ArgumentParser(description='Generate pickle files with pd.DF cutflows and upper limits from a MA5 Recast mode output')
	parser.add_argument('SAF', metavar='SAF_dir_path', help='Path to the Analysis directory containing MA5 results folders') #required=True if not positional
	parser.add_argument('output', metavar='Output_dir_path', help='Path to the output directory to write the pickle files', default=".")
	args = parser.parse_args()

	SAF = args.SAF
	output = args.output
	
	results = [] # Initialize list for UL results and dictionary for Cutflow output
	SRs = {}
	ctau_pat = re.compile(r'\s+1000005\s+([0-9]+\.[0-9]+e.[0-9]+).*wsd3') #Compile regex patterns once to parse parameters from MG5 output for each point
	msb_pat = re.compile(r'\s+1000005\s+([0-9]+\.[0-9]+e.[0-9]+).*')
	mneu1_pat = re.compile(r'\s+1000022\s+([0-9]+\.[0-9]+e.[0-9]+).*')
	xsec_pat = re.compile(r'[0-9]+\s+(.*)\s+[0-9]+.*')
	print("Arguments read: %s and %s" %(SAF, output))
	write_path = Path(args.output) #Initialize path to folder to write results, and create any parents if needed
	write_path.mkdir(parents=True, exist_ok=True)

	for base_path in glob.glob(args.SAF+"/**/", recursive=False):

		in_path=Path(base_path) / "Input" # Initialize paths to Input and Output dirs in the MA5 results folder for the point
		out_path=Path(base_path) / "Output"
		
		param_path = next(Path(next(in_path.glob("*.list")).read_text().strip()).parent.glob("*banner.txt"))       # Get read .list file with path to all input hepmc files
		xsec_path = next(Path(next(in_path.glob("*.list")).read_text().strip()).parent.glob("*merged_xsecs.txt"))  # Assumes 1 dataset (point) per MA5 run, generated by MG5
		xsec = float(re.search(xsec_pat,xsec_path.read_text())[1])                                                 # with 1j to generate *merged_xsecs.txt files
		#print(param_path)
		
		msb=-1
		mneu1=-1
		ctau=-1
		
		with param_path.open('r') as paramfile:
			for line in paramfile:          #Loop over file to match the regex patters line by line, stops once all are matched for performance
				if msb==-1:
					msb_match = re.search(msb_pat, line)
				if mneu1==-1:
					mneu1_match = re.search(mneu1_pat, line)
				if ctau==-1:
					ctau_match = re.search(ctau_pat, line)
				if msb_match is not None:
					msb = float(msb_match[1])
					#print("%s" %(msb))
				if mneu1_match is not None:
					mneu1 = float(mneu1_match[1])
					#print("%s" %(mneu1))
				if ctau_match is not None:
					ctau = 6.582e-16/float(ctau_match[1])
					#print(ctau_match[0])
				#print(re.search('.*1000005.*')[0])
				if msb!=-1 and mneu1!=-1 and ctau!=-1:
					break
			#print("msb: %1.2f\tmneu1: %1.2f\tctau(m): %1.2e\txsec: %.6e" %(msb,mneu1,ctau,xsec))
		
		CLpath = out_path / "SAF" / "CLs_output_summary.dat" #Initialize path to the CLs summary file, and reads it with CSV reader
		CLdf = pd.read_csv(CLpath, header=None,skiprows=1, sep="\s+\|\|\s+|\s+|\|\|", names=["Point", "Analysis", "SR", "Sig95(exp)", "Sig95(obs)", "Eff", "Unc"], engine='python')

		CLaux = CLdf[CLdf["SR"].str.contains("\[SL\].*[0-9]$",regex=True,na=False)] # Matches the limits from the combined regions and extracts it; -1 corresponds to no limit observed
		CLaux_dR = CLdf[CLdf["SR"].str.contains("\[SL\].*_dR",regex=True,na=False)]
		upLim = CLaux.loc[CLaux["Sig95(obs)"] > 0, "Sig95(obs)"].min()
		upLim_dR = CLaux_dR.loc[CLaux_dR["Sig95(obs)"] > 0, "Sig95(obs)"].min()

		if upLim!=upLim: # Quick check if upLim is nan due to all Sig95(obs) values being -1
			upLim=-1
		if upLim_dR!=upLim_dR:
			upLim_dR=-1

		results.append({'msb':msb, 'mn1':mneu1, 'deltaM':msb-mneu1, 'ctau(m)':ctau, 'xsec':xsec, 'ul0':upLim, 'ul_dR':upLim_dR}) #Append dict with extracted results to build DF later
		
		cutflows = glob.glob(base_path+"/**/Cutflows/*.saf", recursive=True) #Find all Cutflows in SAF files

		for file in cutflows:
			cutflow_file = Path(file)
			#print(cutflow_file)
			SRname = cutflow_file.name.replace(".saf","") # Parse SR name from filename
			aux = {'msb' : msb, 'mn1' : mneu1}    #Initialize dict with the mass values identifying the point
			#print(SRname)
			#print(cutflow_file)
			reader = sfr.read_SAF(cutflow_file) # Read cutflow file and extract cuts and surviving events
			cuts = sfr.get_cuts(reader)
			events = sfr.get_entries(reader)
			#print("%s\t%s" %(cuts[0],events[0]))
			#for cut,nevent in zip([cuts[0]],[events[0]]):
			#	print("%s\t%s" %(cut,nevent))
			aux.update(dict(zip(cuts,events))) # Updates dict with cutflow
			#if "_dR" in SRname:
			#	SRsdR.append(aux)
			#else:
			#	SRs.append(aux)
			#pickle_path = write_path / (SRname + ".pcl")
			#pd.DataFrame([aux]).to_pickle(pickle_path)
			if SRname not in SRs:    # Check if SR is already in the dictionary; if not, initialize it
				SRs[SRname] = []
			SRs[SRname].append(aux) # Append cutflow dict to output list to build DF later
		#print("Pickles saved in %s" %(write_path.parent / (write_path.name + "_dR.pcl")))
		#pd.DataFrame(SRs).to_pickle(write_path.parent / (write_path.name + ".pcl"))
		#pd.DataFrame(SRsdR).to_pickle(write_path.parent / (write_path.name + "_dR.pcl"))

	pickle_path = write_path / "cms_exo_19_010.pcl"
	results_df = pd.DataFrame(sorted(results, key=lambda d: d['msb'])).to_pickle(pickle_path) # Builds the Exclusion DF and saves it to pickle file in the output folder

	for sr,cutflow in SRs.items():
		pickle_path = write_path / (sr+".pcl")
		pd.DataFrame(sorted(cutflow, key=lambda d: d['msb'])).to_pickle(pickle_path) # Builds the Cutflow DF and saves it to pickle file in the output folder for each signal region
	print("Parsing has finished!")
